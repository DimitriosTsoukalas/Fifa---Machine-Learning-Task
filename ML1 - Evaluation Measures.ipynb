{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML1 - Evaluation Measures\n",
    "\n",
    "In this experiment, I am using the iris dataset and train a normal Logistic Regression Algorithm.\n",
    "\n",
    "In the following code, there is NO exploratory data analysis, pre-processing, data cleansing or data transformation. The goal here is only to examine the different Evaluation Measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X,y = iris.data, iris.target\n",
    "\n",
    "# With Python version 3, a warning message pops up when I use the Logistic Regression. I prefered to mute that.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Split Data Set\n",
    "\n",
    "One way to proceed with the machine learning task is via **spliting** the dataset to **training** and **test** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size= 0.3)\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: K fold\n",
    "\n",
    "Another way is to make cross validation tests. We define the number of folds to be made (cv = kfold) and the way that random rows are selected (seed).\n",
    "\n",
    "There are different metrics that are presented in a typical classification problem but some of them are for binary and others for multiclass machine learning tasks. In this experiment, MULTICLASS accuracy metrics are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.962 with Standard Deviation: 0.047\n",
      "Log Loss: -0.361 with Standard Deviation: 0.067\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "model = LogisticRegression()\n",
    "\n",
    "#Classification Accuracy\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print((\"Accuracy: %.3f with Standard Deviation: %.3f\") % (results.mean(), results.std()))\n",
    "\n",
    "#Logarithmic Loss\n",
    "scoring = 'neg_log_loss'\n",
    "results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print((\"Log Loss: %.3f with Standard Deviation: %.3f\") % (results.mean(), results.std()))\n",
    "\n",
    "#ROC AUC Curve\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "scoring = 'roc_auc'\n",
    "#results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Accuracy:** It counts how many of the predictions are correct. It works with both binary and multiclass tasks.\n",
    "- **Prcision:** It is about of being precise when guessing. It tracks the percentage of times, when forecasting a class, tha a class was right. If for example you have diagnosed 10 patients with cancer and 9 actually are ill, it means that the precision is 90%.\n",
    "- **Recall:** It reports, among an entiry class, your percentage of correct guesses. If for example there are 20 patients with cancer and you have diagnosed only 9 out of 20, it means that recall is 45%.\n",
    "- **F1 score:** It is considered a great metric that combines Precision and Recall.\n",
    "\n",
    "\n",
    "- **ROC AUC:** It is useful when we want to order the classification according to the probability of being correct. The curve will sort cases starting from the most likely to have cancer to those least likely to have cancer. The curve is higher when the ordering is good and low when it it is bad. If the model has a high ROC AUC, you need to check the most likely ill patients. ROC AUC Curve is only good for binary classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0 12  3]\n",
      " [ 0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(y_test, predicted)\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      0.80      0.89        15\n",
      "           2       0.83      1.00      0.91        15\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        45\n",
      "   macro avg       0.94      0.93      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "report = classification_report(y_test, predicted)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
